\chapter{Project Implementation}
\label{ch:implementation}

In this chapter, the implementation will be demonstrated with code snippets from the source code of the project. Firstly, the implementation to enable download and visualisation or protein models will be explained step-by-step. Secondly, the method for creating new proteins will be discussed. Last but not least, the chapter will elaborate on the implementation of interactive elements in the app.

\section{Download and Visualisation of Protein Models}
Due to its complexity, the process to download and visualise protein models will be explained in five steps.
\subsection{Step 1: Set up Core Data}
First, Core Data is set up by adding a new \emph{Data model} from the \emph{Core Data} section. In this app, the database has only one \emph{Entity} Protein which has two \emph{attributes}: \emph{name} and \emph{location}, defined in type \emph{String} as in Figure \ref{fig:datamodel}. Core Data Stack and Core Data Saving Support need to be added to the \emph{App Delegate} if not automatically generated by Xcode as shown in Figure \ref{fig:coredatacode}. After that, two subclasses must be created where Protein is defined as a public class in \emph{NSManagedObjectSubclass} and function \emph{fetchRequest} is defined as a public class in the extension of Protein. In these subclasses, the attributes of \emph{name} and \emph{location} are also declared as public variables (refer to Figure \ref{fig:subclass}).
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/coredatacode.png}
	\caption{Core Data Stack and Core Data Saving Support}
	\label{fig:coderatacode}
\end{figure}

 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/subclass.png}
	\caption{NSManagedObject subclass}
	\label{fig:subclass}
\end{figure}

\subsection{Step 2: Download from RCSB PDB and save the PDB file locally}
\subsubsection{Download PDB file from RCSB PDB}
Donwload is the critical function in this process. The function is split into two functions: \emph{getDownloadURL} and \emph{download}.

In the function \emph{getDownloadURL} (Figure \ref{fig:download1}), the URL to the source file is created. After observing how files are downloaded from RCSB, a URL pattern was found. Instead of using the \emph{GET} method in \emph{action form}, RCSB allows downloading the PDB files directly from a URL. The structure of the URLs are the same for all of the different PDB files, starting with the same path. The file name is the only part that needs to be changed. The file name is set as the protein's name. With this logic, the URL to the source file was constructed using the parameter as the user-input-text to change the file name accordingly. After creating the URL to the download files, the download function is called with two arguments of \emph{URL} and \emph{parameters} where URL is the to-be constructed URL and parameter is the user's inputted protein name.
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/download1.png}
	\caption{Function getDownloadURL}
	\label{fig:download1}
\end{figure}

The code snippet in Figure \ref{fig:download2} shows the download function that was called by \emph{getDownloadURL}. In this function, the code uses \emph{URLSession} and \emph{downloadTask()} to generate the download task. \emph{URLSession} provides an API for downloading and uploading data to specified URLs. This API helps perform background downloads. In this code, \emph{default} type for \emph{URLSession} is used instead of \emph{shared} because it allows more freedom of configuration. \emph{URLSessionConfiguration} defines the behaviour policies when the app downloads data from the server. There are a few types of \emph{URL Session Tasks}. In this app, \emph{downloadTask} is used as it retrieves data in the form of a file and supports background downloads. 
It is important to take note of the status code returned by \emph{HTTPURLResponse} as it can allow us to address the different possible errors appropriately. The two most common errors are: the file does not exist (status code 404), and the connection to the server was interrupted (status code 500). 

\subsubsection{Save the file in Document Directory}
When the code performs its download task, the file is stored in a temporary location, as called in the code \emph{temporaryURL} (Figure \ref{fig:download2}, line 402). To save the file locally, the file should be moved to a permanent location in the \emph{Document Directory}, using an \emph{absolute path}. To achieve this, \emph{File Manager} was used, in which the destination folder was allocated to \emph{.documentDirectory}, under \emph{userDomainMask} (Figure \ref{fig:download2}, line 408). Stating the path to the \emph{Document Directory} is not enough to make \emph{an absolute path} as the whole URL to the file must be indicated. Therefore, the format of the file that will be downloaded is specified in Figure \ref{fig:download2}, line 409 as \emph{destinationURL} by adding the path components including: the inputted protein's name  as file name and \emph{.pdb} as file extension. For example, if the user domain name is abc123, the proteinID is "6MK1", the path to the downloaded PDB files would be "abc123/Documents/6MK1.pdb". After this absolute path was created, the downloaded file in the temporary location can be moved to the permanent location in \emph{Document Directory} using \emph{FileManager.default.moveItem}. 

 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/download2.png}
	\caption{Function download}
	\label{fig:download2}
\end{figure}


The alternative way is to move the downloaded file into the main app bundle as shown in Figure \ref{fig:movetobundle}. The directory of the main app's bundle is created and the file can be moved by the same \emph{moveItem()} method. In the source code, this alternative way is disabled. The reason for this was previously explained: if all the downloaded files are saved into the main app's folder, the app will have to load them every time the files are loaded, making the app heavy and slow. 
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/movetobundle.png}
	\caption{Alternative: Move downloaded files to main app's folder}
	\label{fig:movetobundle}
\end{figure}


\subsection{Step 3: Assign downloaded files to Core Data}
Firstly, the \emph{context} is declared by \emph{persistentContainer} and the \emph{proteinManagedObject} is declared and initialised as \emph{nil}. 
Then, the function to save \emph{proteinManagedObject} \emph{context} is called inside of the \emph{do} action in the download function (Line 433 -Figure \ref{fig:download2}). The function to save context is displayed in Figure \ref{fig:savefunc}. 
When the download is successful, the attributes \emph{name} and \emph{location} are saved into the Protein \emph{Entity} as a \emph{String} type. 
Since \emph{proteinManageObject} is a global variable, it can be accessed anywhere in the code. 
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/savefunc.png}
	\caption{Save and Assign downloaded file to attributes in Core Data}
	\label{fig:savefunc}
\end{figure}

\subsection{Step 4: Convert PDB file to Collada file}
After the \emph{.pdb} file is downloaded and saved to \emph{Document Directory}, it must be converted to a \emph{.dae} file because ARScene only allows displaying Collada models to its AR Scene. 
A few solutions were used to solve this problem. One of those is to borrow the `PDB to Collada conversion' from UCSF Chimera. Since Chimera was written in Python, its scripts could be run in Swift since Python has a C interface API. However, the challenge was that Chimera is not compatible with iOS, so this solution could not be used. 

OpenBabel was another solution that was investigated . Unfortunately, OpenBabel is only compatible with Android and MacOS, not iOS and in effect was not able to be implemented. 

RCSB PDB published an article on the releasing of their mobile version in 2015 which can help visualise the PDB file on both iOS and Android, however, as of 2020, it was no longer available on the App Store.

This, therefore remains an outstanding issue of the app.

\subsection{Step 5: Fetch and Visualise PDB files}

Data saved into the Core Data can be fetched using \emph{NSFetchRequestResult} and can be displayed easily using the attributes assigned in Core Data. Once the PDB file is converted, the resulting Collada file can be displayed using the function shown in Figure \ref{fig:displayreal}. 
After some trials, all the models after being converted to Collada files are quite large which cannot fit on the screen and need to be scaled down. Each of these models consist of many nodes, however, \emph{node\_1} is always the main node for the whole model. Therefore, in the function (Figure \ref{fig:displayreal}, line 201 - 202), \emph{node\_1} is set as a variable and is scaled down. This assures that in the future, after being ownloaded and converted, the models can always be display completely on screen. 
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/displayreal.png}
	\caption{Function to display protein after being converted into Collada models}
	\label{fig:displayreal}
\end{figure}

As the app does not yet have a functioning “PDB to Collada' converter, already converted protein files in Collada format are made available to demonstrate the visualisation step.

\section{Create new Protein Models}
\subsection{Step 1: Import and name models}
ProteinAR uses ARKit and SceneKit to load the models. For this to happen, models need to be imported. Firstly, a new directory \emph{Scene Catalogue} must be made. In this project, the directory is named “Combinations”. Imported models are in \emph{.dae} format, however, to improve loading times for SceneKit, they are converted to \emph{.scn} type. The files are named according to the polypeptide chains that make them in the order that they make them. See Figure \ref{fig:combinations} for more details as well as some examples.
 \begin{figure}[!htp]
	\centering
	\includegraphics[scale=0.7]{images/combinations.png}
	\caption{Combinations of polypeptide chains stored in a folder}
	\label{fig:combinations}
\end{figure}
This naming convention makes it easy to pass arguments and load models in the upcoming functions. To make the models clearer, Phong shading is used and the colour of each model is assigned randomly. 

\subsection{Step 2: Add polypeptide function}
Figure \ref{fig:addProteinfunc} shows the function used to add polypeptides to the screen. In this function, the argument is pre-defined as a \emph{String} type and has a name matching the \emph{protein's name}. In \emph{ARKit}, \emph{SCNScene} is used to load the 3D models. Since all the models have the same format (inside the “Combinations.scnassets” directory with “.scn” extension), the models will easily be called by passing the names of the protein as arguments each time a Polypeptide button is pressed, as shown in Figure \ref{fig:polypeptidefunc}. 
This function also adds a camera node to the screen at the position of (0, 0, 0) as well as fixes model position using \emph{SCNVector3} to ensure that the models will always appear in front of the camera. One problem that users might encounter with using AR technology is that the space is infinite so models may be loaded off-screen. It is therefore important to ensure the position of the loaded models are visible. This function also uses \emph{eulerAngles} with a random \emph{SCNVector3} to ensure that every time new models are loaded they are at the same position but with different orientations so they do not perfectly overlap each other. This makes it clear to user that a new model has been added.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/addProteinfunc.png}
	\caption{Function to add protein to the screen}
	\label{fig:addProteinfunc}
\end{figure}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/polypeptidefunc.png}
	\caption{Actions of each of the polypeptide button}
	\label{fig:polypeptidefunc}
\end{figure}


\subsection{Step 3: Create new protein}
To simplify the process, the protein combinations are not generated by the code but instead loaded from the “Combinations” folder and displayed. To create a smooth transition and generate the feeling of joining the polypeptide chains, the process has three minor steps. 

\subsubsection{Clear everything off the screen}
Clearing the screen will make the transition to a new model more natural. 
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/clearScreen.png}
	\caption{Function to clear models off the screen}
	\label{fig:clearScreen}
\end{figure}

As the models are added to the screen as nodes (model node, camera node, light node), simply removing all the nodes from \emph{ParentNode()} will clear the screen entirely.

\subsubsection{Load a combination model}
In Figure \ref{fig:polypeptidefunc}, it is shown that every time a button is pressed, besides loading a model onto the screen, it does something else. An empty array is declared in the beginning and every time a button is pressed, a value is added to the array. For example, when the \emph{fCoil button} is pressed, “fCoil” is appended to the array. The values in the array will then be concatenated using \emph{array.joined()} to create the combination name as shown in Figure \ref{fig:createProtein}. 
If the combination exits, the model will be displayed on screen. Similar to the \emph{addProtein} function, to ensure the models appear in front of the camera, camera and protein are added at a fixed position using \emph{cameraNode} and \emph{proteinNode}. Since the models generated from PDB file are large and cannot fit on the screen, they are scaled down when loaded. 

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/createProtein.png}
	\caption{Function to create a new protein}
	\label{fig:createProtein}
\end{figure}

\subsubsection{Display 3D text}
These functions are called inside of the function \emph{createProtein}. If a user-generated combination is valid, together with the model, the 3D text “Congratulations” will be loaded, followed by the names of the polypeptides in order of input. If the combination is invalid, no model will be loaded and instead, only the text “Sorry”! The combination of (\emph{user-pressed-buttons}) cannot be made" will appear. As the function to load text is quite simple and similar to loading models, the code is not shown here. See Appendix A for the full code of this function. 

\section{Interactive elements}
\subsection{Interacting with Protein Models using three gestures}
ARKit is a very powerful framework as it enables gesture interaction. To do this, the gestures must be dragged onto the \emph{Main storyboard} from the built-in library. The three gestures used in ProteinAR are \emph{Pinch Gesture} , \emph{Rotation Gesture} and \emph{Pan Gesture}. The gestures are initiated by \emph{.state: .change}. In ProteinAR, the goal for setting interactions is that the interactions cover the whole screen. This is why the area of enabling gesture is set as \emph{SCNView} while \emph{hitTest} is used to run the gesture. 

The \emph{Pinch Gesture} function shown in figure \ref{fig:pinch} uses \emph{SCNVector3} to change the float values of x, y, and z. Using the two fingertips, users can zoom in and out on the models. 
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/pinch.png}
	\caption{Pinch Gesture function}
	\label{fig:pinch}
\end{figure}
The other two functions for rotating and panning respectively are similar to pinch gesture, with the defining characteristics of \emph{.state} being initialised by \emph{.changed}. These other two functions will not be displayed here. The codes can be found in the Appendix A . 

\subsection{Other interactive elements}
Although it is not a requirement of the app, a more interactive display makes for a presentable UI, so, additional gestures and touches were added to the app. While this may be counterintuitive, the additions improve overall user experience. 

\subsubsection{Gesture Recognizer button}
For the \emph{Record} button, the two gestures of \emph{Tap} and \emph{Long press} were added. Among several alternative methods, creating two objective-C functions was the simplest solution. For this to work, the button should not be connected to the code as an action, but as an outlet. Then, in the \emph{viewDidLoad()}, the \emph{GestureRecognizer} can be added to the outlet as shown in Figure \ref{fig:taplong}. The \emph{GestureRecognizer} function is handled in objective C code (refer to Figure \ref{fig:objcfunc})
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/taplong.png}
	\caption{Add Gesture Recognizer to Button outlet}
	\label{fig:taplong}
\end{figure}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/objcfunc.png}
	\caption{Objective-C functions to handle Gesture Recognizer}
	\label{fig:objcfunc}
\end{figure}

\subsubsection{Dismiss subview and keyboard}
When a user finishes reading the guidelines on \emph{Help Screen View} or finishes inputting in the \emph{textFied}, the sub-screen and the keyboard should be dismissed. For the \emph{Help Screen View}, the solution was to use \emph{UITouch}. This is set so that if a user touches any place that is not the \emph{Help Screen View}, the view will be hidden. 

For the keyboard, it can usually be set with \emph{textFieldShouldEndEditting} after specifying \emph{TextFieldDelegate} in the class. However, in ProteinAR, since the whole screen is covered by \emph{UIGesture}, this did not work. The solution was to set the \emph{Return} key as a \emph{Done} key in \emph{viewDidLoad} and then use the function of \emph{textFieldShouldReturn} to dismiss the keyboard, as shown in Figure \ref{fig:dismiss}.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/dismiss.png}
	\caption{Others interactive elements}
	\label{fig:dismiss}
\end{figure}

\section{Summary}
Based on the design, coding solutions were implemented to achieve the three main goals: download and visualise protein models, create new protein models, and add more interactive elements. To enable downloading and visualising protein models, the process was divided into five steps of implementation. Although the direction of the five steps were planned, step four (convert PDB file to Collada file) was not successfully implemented, resulting in the incompletion of the function. For future development, this step should be the main focus. The implementation to achieve creating new proteins and adding more interactive elements were successfully conducted. Although there are much rooms for further development, these functions lay a solid foundation for the app.
