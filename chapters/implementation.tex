\chapter{Project Implementation}
\label{ch:implementation}

\section{Download and Visualise Protein Models}
Because of its complexity, the process to download and visualise protein models will be explained in five steps.
\subsection{Step 1. Set up Core Data}
First, Core Data is set up by adding a new \emph{Data model} from the \emph{Core Data} section. It is important to add-on the \emph{App delegate} if \emph{Core Data} was added in later in the project because Xcode will not automatically generate those delegate and the database will not be set up. In this app, the database has only one entity with the name of \emph{Protein} which has two \emph{attributes} of \emph{name} and \emph{location}, defined in type \emph{String} as in Figure \ref{fig:datamodel}. After that, \emph{NSManagedObjectSubclass} were created where Protein is defined as a public class in \emph{NSManagedObject} and so as function\emph{fetchRequest}.  In these subclass, the attributes of \emph{name} and \emph{location} are also declared as public variables (refer to Figure \emph{fig: subclass}
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/datamodel.png}
	\caption{Core Data Entity and attributes}
	\label{fig:datamodel}
\end{figure}

 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/subclass.png}
	\caption{NSManagedObject subclass}
	\label{fig:subclass}
\end{figure}

\subsection{Step 2. Download from RCSB PDB using downloadTask()}
\emph{Download} is the main function and since it is a long one, the code snippet will shows parts of the code with modification for easy explanation. The full function code can be found in Appendix A or in the attached source code folder. 

In the code shown in Figure \ref{fig:download1}, the URL to the source file is created. After observing how files are downloaded from RCSB, a pattern was found as instead of using the \emph{GET} method in \emph{action form}, RCSB allows downloading the pdb files directly from an URL. This URL has the structure with the \emph{domain} exactly the same with all the pdb files. The only difference is the file name, which is the protein's name. With this logic, the URL to the source file was constructed using the parameter as the user input text. 
 \begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/download1.png}
	\caption{Download function part 1}
	\label{fig:download1}
\end{figure}

\subsection{Step 3. Assign attributes to Core Data}
\subsection{Step 4. Fetch and Display Data string on Table View}
\subsection{Step 5. Visualising PDB files using API}


\section{Create new Protein Models}
\subsection{Step 1. Import and name models}
Since ProteinAR uses ARKit and SceneKit to load the models on, for importing the models, first, a new file of \emph{Scene Catalogue} must be made. In this project, the folder is named “Combinations”. The provided combination model type was in \emph{.dae}, however, to enable smooth loading for SceneKit, the files are converted into \emph{.scn} type. These are named after the polypeptide chains names and their order in creating the combinations. See Figure \ref{fig:combination} for more details of some examples.
 \begin{figure}[!htp]
	\centering
	\includegraphics[scale=0.7]{images/combinations.png}
	\caption{Combinations of polypeptide chains stored in a folder}
	\label{fig:combinations}
\end{figure}
This naming convention makes it easy to pass as arguments and load models in the following functions. To make the models more appealing, Phong shading was used and the colour of each and every model are randomly picked. 

\subsection{Step 2. Add polypeptide function}
Figure \ref{fig:addProteinfunc} shows the function to add the polypeptides on the screen. In this function, the argument is pre-defined as \emph{String} type and has the name of \emph{name}. In \emph{ARKit}, \emph{SCNScene} is used to load the 3D models. Since all the models have the same format (inside of “Combinations.scnassets” folder and has the “.scn” extension), the models will easily be called by passing the names of the protein as arguments each time a Polypeptide button is pressed, as shown in Figure \ref{polypeptidefunc}. 
The function also add a camera node to the screen at the position of (0, 0, 0) and the position of the models are also fixed by \emph{SCNVector3} to ensure that the models will always appear in front of the camera. One of the tricky things with using AR technology is that the space is infinity and so the models might be loaded in places that cannot be seen. Thus, it seems more user-friendly to make sure the position of the models loaded are visible. The function also uses \emph{eulerAngles} with a random \emph{SCNVector3} to make sure that every time new models are loaded, they might be at the same position, but rotates differently so they do not lay on top of each other perfectly, ensuring users that they have already added the same models more than one time. 
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/addProteinfunc.png}
	\caption{Function to add protein to the screen}
	\label{fig:addProteinfunc}
\end{figure}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/polypeptidefunc.png}
	\caption{Actions happen when users press on each Polypeptide Button}
	\label{fig:polypeptidefunc}
\end{figure}


\subsection{Step 3. Create new protein}
To simplify the process and still keep the fun as users can make a new protein, the combinations of protein are not completely new but instead, loaded from the “Combinations” folder and displayed. To create a smooth transition and generate the feeling of joining the polypeptide chains, the process has three small steps. 

\subsubsection{Clear everything off the screen}
Clearing the screen off will make the transition to a new model feels more approved. 
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/clearScreen.png}
	\caption{Function to clear models off the screen}
	\label{fig:clearScreen}
\end{figure}

As the models are added on the screen as nodes (model node, camera node, light node), simply by removing all the node from \emph{ParentNode()} would enable clearing off the screen.

\subsubsection{Load a combination model}
In Figure \ref{polypeptidefunc}, it is shown that every time a button is pressed, besides loading a model onto the screen, it does something else. An empty array variable was declared in the beginning and every time a button is pressed, a value is added to the array. For example, when \emph{fCoil Button} is pressed, “fCoil” is added to the array. These values in the array will then be joined using \emph{array.joined()} in the function to create protein as shown in Figure \ref{fig:createProtein}. Without any separator, these joined values will become exactly like the names of models in the “Combinations” folder, which enable the code to run and load the models from there. 
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/createProtein.png}
	\caption{Function to create a new protein}
	\label{fig:createProtein}
\end{figure}

\subsubsection{Display 3D text}
These functions are called inside of function \emph{createProtein}. If the combination that user created is valid, together with the model, the text will be loaded with “Congratulations”, and followed by the names of the polypeptides in order. If the combination that user created is invalid, no model would be loaded and instead, only the 3D text will appear with “Sorry”. See Appendix A for this function. 

\section{Interactive elements}
\subsection{The three gestures to interact with Protein Models}
ARKit is a very powerful framework as it cuts off a lot of coding work to enable gesture interaction. To enable gesture interactions, the first step is to drag and drop the gesture on \emph{Main storyboard} from the built-in library. The three gestures used in ProteinAR are \emph{Pinch Gesture} , \emph{Rotation Gesture} and \emph{Pan Gesture}. The gestures are initiated by \emph{.state: .change}. As in ProteinAR, the goal of interaction is the full screen, the area of gesture is set to be \emph{SCNView} and \emph{hitTest} is used to run the gesture. 

In the function to generate \emph{Pinch Gesture} as shown in figure \ref{fig:pinch}, \emph{SCNVector3} is used with changeable (x, y, z) set in float. By using the two fingertips, users can zoom in and zoom out on the models. 
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/pinch.png}
	\caption{Pinch Gesture function}
	\label{fig:pinch}
\end{figure}
The other two functions of rotation and pan gesture are similar to pinch gesture and thus, will not be displayed in code here. The codes can be found in the Appendix A. 

\subsection{Other interactive elements}
Although it is not a compulsory requirement of the app, more interactive display will make the UI more appealing, thus, some other gestures and touches are added in the app. This might not seem very obvious but it improves the user experience.
\subsubsection{Gestures Recognizer for button}
For the \emph{Record} button, the two gestures of \emph{Tap} and \emph{Long press} were added. There might be other methods to do this, however, creating two objective-C functions were the simplest solution. First, the button should not be connected to the code as an action, but an outlet. Then, in the \emph{viewDidLoad()}, \emph{GestureRecognizer} can be added to the outlet as shown in Figure \ref{fig:taplong}. These \emph{GestureRecognizer} needs handling, which will be handled in objective-C’s function (refer to Figure \ref{fig:objcfunc}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/taplong.png}
	\caption{Add Gesture Recognizer to Button outlet}
	\label{fig:taplong}
\end{figure}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/objcfunc.png}
	\caption{Objective-C functions to handle Gesture Recognizer}
	\label{fig:objcfunc}
\end{figure}

\subsubsection{Dismiss subview and keyboard}
When user finishes reading the guidelines on \emph{Help Screen View} or finishing input in the \emph{textFied} of the protein name, these should be dismissed. For the emph{Help Screen View}, the solution was to use \emph{UITouch}. This is set so that if users touches every place that is not the emph{Help Screen View}, the view will be hidden. 
For the keyboard, usually it can just be set with \emph{textFieldShouldEndEditting} after specified \emph{TextFieldDelegate} in the class. However, since in ProteinAR, the whole screen are covered in other \emph{UIGesture}, this did not work. The solution was to set the \emph{Return} key to \emph{Done} key in the \emph{viewDidLoad} and then use the function of \emph{textFieldShouldReturn} to dismiss the keyboard, as shown in Figure \ref{fig:dismiss}.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/dismiss.png}
	\caption{Others interactive elements}
	\label{fig:dismiss}
\end{figure}

